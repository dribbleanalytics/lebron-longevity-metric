{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full data\n",
    "df_all_players = pd.read_csv('final_csv_data/full_nba_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create All-NBA models from full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features and output for model\n",
    "features = ['g', 'mp', 'pts', 'trb', 'ast', 'vorp', 'ws']\n",
    "output = ['all-nba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing\n",
    "train, test = train_test_split(df_all_players, test_size=0.25, random_state=0)\n",
    "\n",
    "xtrain = train[features]\n",
    "ytrain = train[output]\n",
    "\n",
    "xtest = test[features]\n",
    "ytest = test[output]\n",
    "\n",
    "print(\"Training set size: %.0f\" % len(xtrain))\n",
    "print(\"Testing set size: %.0f\" % len(xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that fits model and returns performance metrics\n",
    "def scores(model):\n",
    "    \n",
    "    model.fit(xtrain, ytrain.values.ravel())\n",
    "    y_pred = model.predict(xtest)\n",
    "    \n",
    "    print(\"Accuracy score: %.3f\" % metrics.accuracy_score(ytest, y_pred))\n",
    "    print(\"Recall: %.3f\" % metrics.recall_score(ytest, y_pred))\n",
    "    print(\"Precision: %.3f\" % metrics.precision_score(ytest, y_pred))\n",
    "    print(\"F1: %.3f\" % metrics.f1_score(ytest, y_pred))\n",
    "    \n",
    "    proba = model.predict_proba(xtest)\n",
    "    print(\"Log loss: %.3f\" % metrics.log_loss(ytest, proba))\n",
    "\n",
    "    pos_prob = proba[:, 1]\n",
    "    print(\"Area under ROC curve: %.3f\" % metrics.roc_auc_score(ytest, pos_prob))\n",
    "    \n",
    "    cv = cross_val_score(model, xtest, ytest.values.ravel(), cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy (cross validation score): %0.3f (+/- %0.3f)\" % (cv.mean(), cv.std() * 2))\n",
    "    \n",
    "    cv = cross_val_score(model, xtest, ytest.values.ravel(), cv=3, scoring='recall')\n",
    "    print(\"Recall (cross validation score): %0.3f (+/- %0.3f)\" % (cv.mean(), cv.std() * 2))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "y_rf = scores(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "y_gbc = scores(gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgboost.XGBClassifier(random_state=0)\n",
    "\n",
    "y_xgb = scores(xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy=\"stratified\", random_state=0)\n",
    "\n",
    "y_dummy = scores(dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict historical All-NBA score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that trians models on all data except for year x, then outputs All-NBA probabilities for year x\n",
    "def make_pred(model_list, df):\n",
    "    \n",
    "    df_year_order = df.sort_values(by='season_start')\n",
    "    df_pred_list = []\n",
    "    for year in range(1979, 2019):\n",
    "        df_curr = df_year_order[df_year_order['season_start']==year].reset_index(drop=True)\n",
    "        df_train = df_year_order[df_year_order['season_start']!=year].reset_index(drop=True)\n",
    "        prob_list = []\n",
    "        for i in model_list:\n",
    "            i.fit(df_train[features], df_train[output].values.ravel())\n",
    "            proba = i.predict_proba(df_curr[features])\n",
    "            pos_prob = proba[:, 1]\n",
    "            prob_list.append(pos_prob)\n",
    "        df_curr['pred_all_nba'] = np.mean(prob_list, axis=0)\n",
    "        df_pred_list.append(df_curr)\n",
    "    return pd.concat(df_pred_list).sort_values(by='season_id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_list = make_pred([rf, gbc, xgb], df_all_players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get metrics of leave-one-out predictions\n",
    "def get_loo_metrics(df):\n",
    "    \n",
    "    ytest = df['all-nba'].values\n",
    "    yprob = df['pred_all_nba'].values\n",
    "    ypred = (yprob > 0.5).astype(int)\n",
    "    \n",
    "    print(\"Accuracy score: %.3f\" % metrics.accuracy_score(ytest, ypred))\n",
    "    print(\"Recall: %.3f\" % metrics.recall_score(ytest, ypred))\n",
    "    print(\"Precision: %.3f\" % metrics.precision_score(ytest, ypred))\n",
    "    print(\"F1: %.3f\" % metrics.f1_score(ytest, ypred))\n",
    "\n",
    "    print(\"Log loss: %.3f\" % metrics.log_loss(ytest, np.array([1 - yprob, yprob]).T))\n",
    "\n",
    "    print(\"Area under ROC curve: %.3f\" % metrics.roc_auc_score(ytest, yprob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_loo_metrics(df_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take necessary parts of df, save to csv\n",
    "out_df = df_pred_list[['player', 'player_id', 'age', 'player_season', 'season_start', 'pred_all_nba']]\n",
    "out_df.to_csv('results/all_nba_preds.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
